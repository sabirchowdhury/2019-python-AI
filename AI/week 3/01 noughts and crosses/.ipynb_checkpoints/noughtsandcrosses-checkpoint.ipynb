{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Games and Adversarial Search\n",
    "## Noughts and Crosses\n",
    "### Introduction\n",
    "\n",
    "<figure>\n",
    "<img src=\"images/noughtsandcrosses.png\", width=200 align=\"right\">\n",
    "    <figcaption></figcaption>\n",
    "</figure>\n",
    "\n",
    "Noughts and Crosses, or Tic-Tac-Toe is a simple game for two players played on a 3x3 grid. One player takes O (noughts) and the other takes X (crosses). The players alternate placing their symbol in a cell on the grid until one of the gets three in a row, winning the game.\n",
    "\n",
    "The game is well known for having a relatively easy to learn optimal strategy, which will lead to a draw if followed by both players."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Minimax Search\n",
    "Minimax is a simple search algorithm which can determine optimal play assuming the opponent is going to play perfectly. This assumption usually works even against suboptimal players – good moves are still good!\n",
    "\n",
    "The full version of the algorithm is a depth first search of the entire state space for a 2-player game. First we model the problem such that high rewards benefit us and low (or negative) rewards benefit our opponent: so we are looking to maximise the reward, and our opponent is looking to minimise the reward. \n",
    "\n",
    "Assume we are player 1. On a given turn, look at the reward for each action, and take the maximum one. How do we find the reward for each action? Well, we look at what player 2 would do in response following the same logic: look at every action available to them, and choose the one that minimises the reward. Of course this *also* requires knowing the rewards, for which we might need to look at what player 1 would do in response to each action. And so on. Eventually, one of the moves will end the game, and the reward can be fed back up the search tree.\n",
    "\n",
    "Here is the pseudocode for minimax from Russell and Norvig (p. 166): <br />\n",
    "<img src=\"./images/minimax.png\" width=500 />\n",
    "\n",
    "## Example\n",
    "In the cell below you will find two classes. \n",
    "* The first class, `NoughtsAndCrosses`, is for the game state and rules – you can skim through the code if you want to see how it works, but hopefully it will be clear enough from the various names. \n",
    "* The second class, `MinimaxAgent`, is the important one. This is the one that contains the recursive function which you should read carefully to better your understanding of the algorithm.\n",
    "\n",
    "Note that the cell will not immediately produce any output when it is run, but there is code further down the page which shows a demonstration (as always make sure you have actually run the following cell first)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import copy\n",
    "\n",
    "\n",
    "class NoughtsAndCrosses:\n",
    "    def __init__(self):\n",
    "        self.EMPTY = ' '\n",
    "        self.NOUGHT = 'O'\n",
    "        self.CROSS = 'X'\n",
    "        self.DRAW = 'draw'\n",
    "\n",
    "        self.next_player = self.CROSS\n",
    "        self.flip_player = {self.CROSS: self.NOUGHT, self.NOUGHT: self.CROSS}\n",
    "\n",
    "        self.board = np.array([[self.EMPTY for _ in range(3)] for _ in range(3)])\n",
    "\n",
    "    def valid_move(self, row, col):\n",
    "        return self.board[row, col] == self.EMPTY\n",
    "\n",
    "    def move(self, row, col):\n",
    "        \"\"\"Returns a *copy* of this state with the specified move\"\"\"\n",
    "        if not self.valid_move(row, col):\n",
    "            raise ValueError(\"Position not empty\")\n",
    "\n",
    "        state = copy.deepcopy(self)\n",
    "        state.board[row, col] = state.next_player\n",
    "        state.next_player = state.flip_player[state.next_player]\n",
    "        return state\n",
    "\n",
    "    def winner(self):\n",
    "        for i in range(3):\n",
    "            if np.all(self.board[i, :] == self.board[i, 0]) and self.board[i, 0] != self.EMPTY:\n",
    "                return self.board[i, 0]\n",
    "            if np.all(self.board[:, i] == self.board[0, i]) and self.board[0, i] != self.EMPTY:\n",
    "                return self.board[0, i]\n",
    "        if self.EMPTY != self.board[0, 0] and self.board[0, 0] == self.board[1, 1] and self.board[1, 1] == self.board[2, 2]:\n",
    "            return self.board[1, 1]\n",
    "        if self.EMPTY != self.board[2, 0] and self.board[2, 0] == self.board[1, 1] and self.board[1, 1] == self.board[0, 2]:\n",
    "            return self.board[1, 1]\n",
    "        if np.all(self.board != self.EMPTY):\n",
    "            return self.DRAW\n",
    "        return False\n",
    "\n",
    "    def actions(self):\n",
    "        row, col = np.nonzero(self.board == self.EMPTY)\n",
    "        return [(r, c) for r, c in zip(row, col)]\n",
    "\n",
    "\n",
    "class MinimaxAgent:\n",
    "    def __init__(self, verbose=False):\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def next_move(self, state=NoughtsAndCrosses()):\n",
    "        player = state.next_player\n",
    "\n",
    "        best_action = None\n",
    "        best_value = -1 * math.inf\n",
    "        for action in state.actions():\n",
    "            new_state = state.move(action[0], action[1])\n",
    "            action_value = self.get_value(new_state, player, get_min=True)\n",
    "            if self.verbose:\n",
    "                print(action_value, end=\" \")\n",
    "            if action_value > best_value:\n",
    "                best_action = action\n",
    "                best_value = action_value\n",
    "        if self.verbose:\n",
    "            print()\n",
    "        return best_action\n",
    "\n",
    "    def get_value(self, state, player, get_min):\n",
    "        \"\"\"If get_min is set to true, returns the minimum value, otherwise the maximum value\"\"\"\n",
    "        other_player = state.flip_player[player]\n",
    "\n",
    "        winner = state.winner()\n",
    "        if winner == player:\n",
    "            return 1\n",
    "        elif winner == other_player:\n",
    "            return -1\n",
    "        elif winner == state.DRAW:\n",
    "            return 0\n",
    "\n",
    "        best_value = math.inf\n",
    "        if not get_min:\n",
    "            best_value *= -1\n",
    "\n",
    "        for action in state.actions():\n",
    "            new_state = state.move(action[0], action[1])\n",
    "            action_value = self.get_value(new_state, player, get_min=not get_min)\n",
    "            if not get_min and action_value > best_value \\\n",
    "                    or get_min and action_value < best_value:\n",
    "                best_value = action_value\n",
    "\n",
    "        return best_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demonstration\n",
    "Run the cell below to play a game against this AI.\n",
    "\n",
    "There is an additional agent class called `HumanAgent`, which will ask the user for input on the command line to determine what move to make. Then there is a function which plays a game of noughts and crosses. By default the game is Human vs AI (with the AI going second), but you can manually change the values for `player1` and `player2` to change this. \n",
    "\n",
    "Just a warning: the minimax algorithm generates every possible game when it is asked to go first – expect it to take a long time to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HumanAgent:\n",
    "    def next_move(self, state):\n",
    "        while True:\n",
    "            try:\n",
    "                print(\"What's your next move? In format row,col\")\n",
    "                move = input(\">\")\n",
    "                move = move.split(',')\n",
    "                move = int(move[0]), int(move[1])\n",
    "                if not state.valid_move(move[0], move[1]):\n",
    "                    print(\"Space must be empty.\")\n",
    "                else:\n",
    "                    return move\n",
    "            except ValueError:\n",
    "                print(\"Please enter valid space as row,col between 0,0 and 2,2\")\n",
    "\n",
    "\n",
    "def run_game(player1=HumanAgent(), player2=MinimaxAgent()):\n",
    "    state = NoughtsAndCrosses()\n",
    "    print(state.board)\n",
    "    while not state.winner():\n",
    "        move = player1.next_move(state)\n",
    "        state = state.move(move[0], move[1])\n",
    "        print(state.board)\n",
    "        if state.winner() == state.CROSS:\n",
    "            print(\"Player one wins!\")\n",
    "            return\n",
    "        elif state.winner() == state.DRAW:\n",
    "            print(\"It's a draw.\")\n",
    "            return\n",
    "\n",
    "        move = player2.next_move(state)\n",
    "        state = state.move(move[0], move[1])\n",
    "        print(state.board)\n",
    "        if state.winner() == state.NOUGHT:\n",
    "            print(\"Player two wins!\")\n",
    "            return\n",
    "        elif state.winner() == state.DRAW:\n",
    "            print(\"It's a draw.\")\n",
    "            return\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_game()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extensions\n",
    "Minimax is too inefficient for most real games, it's only just enough to work on noughts and crosses. The minimax implementation above considers 549,945 states to calculate its opening move. We could cut this significantly by optimising for repeated states and symmetry of the board. But even with optimisations like these, the search graph for chess has about $10^{40}$ distinct states, or 10,000,000,000,000,000,000,000,000,000,000,000,000,000.\n",
    "\n",
    "Still, minimax is a good foundation on which to build other optimisations:\n",
    "* Alpha-beta pruning\n",
    "* Table lookups\n",
    "* Evaluation functions\n",
    "\n",
    "## Alpha-Beta Pruning\n",
    "Alpha-beta pruning is a technique which can cut the search space of minimax roughly in half with no loss of optimality. Consider this example game from Russel and Norvig (p. 164-168): <br />\n",
    "<img src=\"images/game.png\" width=400 />\n",
    "\n",
    "Now suppose we are searching using minimax and we reach this point in the tree: <br />\n",
    "<img src=\"images/gameab.png\" width=400 />\n",
    "\n",
    "Notice that if the MAX player choose action $a_1$, then they are guaranteed a result of 3, because this is the lowest value for the MIN player. So the MAX player knows their *minimum* result is 3.\n",
    "\n",
    "Now while exploring the MIN player's actions after MAX takes action $a_2$, we realise that MIN has the option of picking an action that will result in a reward of 2, the action $c_1$. We can now rule out action $a_2$ from the perspective of the MAX player, *without* having to check the rewards for $c_2$ or $c_3$. The MAX player knows $a_1$ is a better option than $a_2$, because if $c_2$ or $c_3$ give a better result than 3, the MIN player will not choose it, and we do not care if $c_2$ or $c_3$ give a *worse* result than 2, because 2 is already worse than the 3 we can get from taking $a_1$.\n",
    "\n",
    "## Task\n",
    "Use the pseudocode below (Russel and Norvig p. 170) to adapt the MinimaxAgent to use alpha-beta pruning. If you are stuck, read this section in the textbook for more details. \n",
    "\n",
    "Some skeleton code is provided in the cell below, look for the lines which say `### YOUR CODE HERE`. <br />\n",
    "<img src=\"./images/abpseudo.png\" width=500 />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ABMinimaxAgent:\n",
    "    def __init__(self, verbose=False):\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def next_move(self, state=NoughtsAndCrosses()):\n",
    "        player = state.next_player\n",
    "\n",
    "        best_action = None\n",
    "        best_value = -1 * math.inf\n",
    "        for action in state.actions():\n",
    "            new_state = state.move(action[0], action[1])\n",
    "            action_value = self.get_value(new_state, player, get_min=True)\n",
    "            if self.verbose:\n",
    "                print(action_value, end=\" \")\n",
    "            if action_value > best_value:\n",
    "                best_action = action\n",
    "                best_value = action_value\n",
    "        if self.verbose:\n",
    "            print()\n",
    "        return best_action\n",
    "\n",
    "    def get_value(self, state, player, get_min, alpha=-math.inf, beta=math.inf):\n",
    "        \"\"\"If get_min is set to true, returns the minimum value, otherwise the maximum value\"\"\"\n",
    "        other_player = state.flip_player[player]\n",
    "\n",
    "        winner = state.winner()\n",
    "        if winner == player:\n",
    "            return 1\n",
    "        elif winner == other_player:\n",
    "            return -1\n",
    "        elif winner == state.DRAW:\n",
    "            return 0\n",
    "\n",
    "        best_value = math.inf\n",
    "        if not get_min:\n",
    "            best_value *= -1\n",
    "\n",
    "        for action in state.actions():\n",
    "            new_state = state.move(action[0], action[1])\n",
    "            action_value = self.get_value(new_state, player, get_min=not get_min, alpha=alpha, beta=beta)\n",
    "            \n",
    "            if not get_min:\n",
    "                ### YOUR CODE HERE\n",
    "            else:\n",
    "                ### YOUR CODE HERE\n",
    "\n",
    "            if not get_min and action_value > best_value \\\n",
    "                    or get_min and action_value < best_value:\n",
    "                best_value = action_value\n",
    "\n",
    "        return best_value\n",
    "    \n",
    "\n",
    "run_game(player1=ABMinimaxAgent(), player2=ABMinimaxAgent())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
